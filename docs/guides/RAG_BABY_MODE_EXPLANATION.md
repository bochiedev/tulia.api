# RAG Workflow Explained (Baby Mode) ğŸ¼

## The Big Picture: What is RAG?

Imagine you're a student taking an open-book exam. Instead of memorizing everything, you can look up answers in your textbooks when you need them. That's basically what RAG does for AI!

**RAG = Retrieval-Augmented Generation**
- **Retrieval** = Looking up information
- **Augmented** = Enhanced/improved
- **Generation** = Creating responses

## The Problem RAG Solves

**Without RAG:**
```
Customer: "What is your return policy?"
AI: "I think it's 30 days... or maybe 14? I'm not sure." âŒ
```

**With RAG:**
```
Customer: "What is your return policy?"
AI: [Looks up in FAQ document]
AI: "According to our FAQ, we offer a 30-day return policy..." âœ…
```

## Where LangChain and Pinecone Fit

Think of building a house:
- **LangChain** = The construction tools (hammer, saw, drill)
- **Pinecone** = The storage warehouse (where you keep materials)

### LangChain = The Smart Tools ğŸ”§

LangChain is like a Swiss Army knife for AI. It provides ready-made tools so you don't have to build everything from scratch.

**What LangChain Does in Our System:**

1. **Text Splitting (Chunking)**
   - **File:** `apps/bot/services/chunking_service.py`
   - **What it does:** Breaks big documents into bite-sized pieces
   
   ```python
   from langchain.text_splitter import RecursiveCharacterTextSplitter
   
   # LangChain's smart text splitter
   splitter = RecursiveCharacterTextSplitter(
       chunk_size=400,      # Each piece is ~400 tokens
       chunk_overlap=50,    # Pieces overlap by 50 tokens
       separators=["\n\n", "\n", ". ", " "]  # Split at paragraphs, sentences, etc.
   )
   ```
   
   **Baby Mode Analogy:**
   - You have a 100-page book
   - LangChain cuts it into 200 small cards (chunks)
   - Each card has 1-2 paragraphs
   - Cards overlap a bit so you don't lose context
   
   **Why?** AI can't read a whole book at once, but it can read small cards!

### Pinecone = The Smart Library ğŸ“š

Pinecone is like a magical library where you can find things by meaning, not just by title.

**What Pinecone Does in Our System:**

1. **Stores Vector Embeddings**
   - **File:** `apps/bot/services/vector_store.py`
   - **What it does:** Stores "meaning fingerprints" of text
   
   ```python
   from pinecone import Pinecone
   
   # Connect to Pinecone
   pc = Pinecone(api_key="your-key")
   index = pc.Index("tulia-rag")
   
   # Store a chunk with its "meaning fingerprint"
   index.upsert(vectors=[
       {
           "id": "chunk_123",
           "values": [0.1, 0.5, 0.3, ...],  # 1536 numbers = meaning fingerprint
           "metadata": {"text": "Our return policy is 30 days"}
       }
   ])
   ```
   
   **Baby Mode Analogy:**
   - Regular library: Find books by title/author
   - Pinecone library: Find books by what they mean
   - You ask: "Tell me about returns"
   - Pinecone finds: "return policy", "refund", "money back" (similar meanings!)

## The Complete RAG Workflow (Step by Step)

### Phase 1: Setup (One Time) ğŸ“¥

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. UPLOAD DOCUMENT                                          â”‚
â”‚    Customer uploads "FAQ.pdf"                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. EXTRACT TEXT                                             â”‚
â”‚    Read PDF â†’ "Our return policy is 30 days..."            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. CHUNK TEXT (LangChain)                                   â”‚
â”‚    Big document â†’ 200 small chunks                          â”‚
â”‚                                                              â”‚
â”‚    Chunk 1: "Our return policy is 30 days..."              â”‚
â”‚    Chunk 2: "You can return items in original..."          â”‚
â”‚    Chunk 3: "Contact customer service for..."              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. CREATE EMBEDDINGS (OpenAI)                               â”‚
â”‚    Convert text â†’ numbers (meaning fingerprints)            â”‚
â”‚                                                              â”‚
â”‚    "return policy" â†’ [0.1, 0.5, 0.3, 0.8, ...]             â”‚
â”‚    (1536 numbers that represent the meaning)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. STORE IN PINECONE                                        â”‚
â”‚    Save chunks + embeddings in vector database              â”‚
â”‚                                                              â”‚
â”‚    Pinecone now has:                                        â”‚
â”‚    - 200 chunks from FAQ.pdf                                â”‚
â”‚    - Each with its meaning fingerprint                      â”‚
â”‚    - Organized by tenant (no mixing!)                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Phase 2: Retrieval (Every Query) ğŸ”

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. CUSTOMER ASKS QUESTION                                   â”‚
â”‚    "What is your return policy?"                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. CREATE QUERY EMBEDDING (OpenAI)                          â”‚
â”‚    Convert question â†’ numbers                               â”‚
â”‚                                                              â”‚
â”‚    "return policy" â†’ [0.12, 0.48, 0.31, 0.79, ...]         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. SEARCH PINECONE                                          â”‚
â”‚    Find chunks with similar meaning fingerprints            â”‚
â”‚                                                              â”‚
â”‚    Query: [0.12, 0.48, 0.31, ...]                          â”‚
â”‚    â†“                                                         â”‚
â”‚    Pinecone compares with all stored chunks                 â”‚
â”‚    â†“                                                         â”‚
â”‚    Returns top 3 most similar:                              â”‚
â”‚    1. "Our return policy is 30 days..." (95% match)        â”‚
â”‚    2. "You can return items in original..." (87% match)    â”‚
â”‚    3. "Contact customer service for..." (75% match)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. ALSO SEARCH DATABASE                                     â”‚
â”‚    Look for products, services, orders                      â”‚
â”‚                                                              â”‚
â”‚    Found: 0 products (not relevant to returns)              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. COMBINE RESULTS                                          â”‚
â”‚    Merge document chunks + database results                 â”‚
â”‚                                                              â”‚
â”‚    Context for AI:                                          â”‚
â”‚    "From FAQ: Our return policy is 30 days..."             â”‚
â”‚    "From FAQ: You can return items in original..."         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 6. SEND TO AI (OpenAI GPT-4)                                â”‚
â”‚    Prompt: "Using this context, answer the question..."     â”‚
â”‚                                                              â”‚
â”‚    Context: [Retrieved information]                         â”‚
â”‚    Question: "What is your return policy?"                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 7. AI GENERATES RESPONSE                                    â”‚
â”‚    "According to our FAQ, we offer a 30-day return         â”‚
â”‚     policy on all items. [Source: FAQ.pdf]"                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 8. SEND TO CUSTOMER                                         â”‚
â”‚    Customer gets accurate answer with source!               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## The Magic of Embeddings (Explained Simply)

**What are embeddings?**
Embeddings are like GPS coordinates for words and sentences.

**Example:**

```
Text: "return policy"
Embedding: [0.12, 0.48, 0.31, 0.79, 0.22, ...] (1536 numbers)

Text: "refund rules"
Embedding: [0.13, 0.47, 0.30, 0.78, 0.21, ...] (very similar numbers!)

Text: "pizza recipe"
Embedding: [0.89, 0.02, 0.65, 0.11, 0.93, ...] (very different numbers!)
```

**Why this works:**
- Similar meanings â†’ Similar numbers
- Different meanings â†’ Different numbers
- Pinecone can find similar numbers super fast!

## Real Code Examples

### 1. LangChain Chunking (apps/bot/services/chunking_service.py)

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

# Create the splitter
splitter = RecursiveCharacterTextSplitter(
    chunk_size=400,           # Each chunk ~400 tokens
    chunk_overlap=50,         # 50 tokens overlap
    separators=["\n\n", "\n", ". ", " "]  # Split smartly
)

# Use it
text = "Our return policy is 30 days. You can return items..."
chunks = splitter.split_text(text)

# Result:
# chunks[0] = "Our return policy is 30 days. You can return items..."
# chunks[1] = "...return items in original packaging. Contact us..."
```

**What LangChain does here:**
- Tries to split at paragraph breaks first
- If chunk too big, splits at sentences
- If still too big, splits at words
- Keeps some overlap so context isn't lost

### 2. Pinecone Storage (apps/bot/services/vector_store.py)

```python
from pinecone import Pinecone

# Connect to Pinecone
pc = Pinecone(api_key="your-key")
index = pc.Index("tulia-rag")

# Store chunks
vectors = [
    {
        "id": "chunk_1",
        "values": [0.1, 0.5, 0.3, ...],  # Embedding (1536 numbers)
        "metadata": {
            "text": "Our return policy is 30 days",
            "document_id": "faq_123",
            "tenant_id": "tenant_456"
        }
    }
]

# Upload to Pinecone
index.upsert(vectors=vectors, namespace="tenant_456")
```

**What Pinecone does here:**
- Stores the embedding (meaning fingerprint)
- Stores metadata (text, document ID, tenant ID)
- Uses namespace for tenant isolation (no mixing!)

### 3. Pinecone Search (apps/bot/services/vector_store.py)

```python
# Search for similar chunks
query_embedding = [0.12, 0.48, 0.31, ...]  # From "return policy" question

results = index.query(
    vector=query_embedding,
    top_k=3,                              # Get top 3 matches
    namespace="tenant_456",               # Only this tenant's data
    include_metadata=True
)

# Results:
# [
#   {
#     "id": "chunk_1",
#     "score": 0.95,  # 95% similar!
#     "metadata": {"text": "Our return policy is 30 days..."}
#   },
#   {
#     "id": "chunk_2",
#     "score": 0.87,  # 87% similar
#     "metadata": {"text": "You can return items in original..."}
#   }
# ]
```

**What Pinecone does here:**
- Compares query embedding with all stored embeddings
- Finds the most similar ones (using cosine similarity)
- Returns top matches with scores
- Only searches within the tenant's namespace

## Why We Use Both

### LangChain = The Prep Cook ğŸ‘¨â€ğŸ³
- Prepares the ingredients (chunks text)
- Uses smart techniques (recursive splitting)
- Makes everything ready for storage

### Pinecone = The Smart Fridge ğŸ§Š
- Stores everything organized
- Finds things by smell/taste (meaning), not just label
- Super fast retrieval (milliseconds!)

## The Full Stack

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        CUSTOMER                              â”‚
â”‚                  "What is your return policy?"               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    AI AGENT SERVICE                          â”‚
â”‚              (apps/bot/services/ai_agent_service.py)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  RAG RETRIEVER SERVICE                       â”‚
â”‚           (apps/bot/services/rag_retriever_service.py)       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                â”‚                â”‚
        â–¼                â–¼                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  DOCUMENTS   â”‚  â”‚   DATABASE   â”‚  â”‚   INTERNET   â”‚
â”‚              â”‚  â”‚              â”‚  â”‚              â”‚
â”‚ LangChain    â”‚  â”‚  PostgreSQL  â”‚  â”‚   Google     â”‚
â”‚ Pinecone     â”‚  â”‚   Queries    â”‚  â”‚   Search     â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
       â”‚                 â”‚                 â”‚
       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  CONTEXT SYNTHESIZER                         â”‚
â”‚           (apps/bot/services/context_synthesizer.py)         â”‚
â”‚                                                              â”‚
â”‚  Merges all results into coherent context                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      OPENAI GPT-4                            â”‚
â”‚                                                              â”‚
â”‚  Generates response using retrieved context                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  ATTRIBUTION HANDLER                         â”‚
â”‚           (apps/bot/services/attribution_handler.py)         â”‚
â”‚                                                              â”‚
â”‚  Adds source citations: [Source: FAQ.pdf]                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        CUSTOMER                              â”‚
â”‚  "According to our FAQ, we offer a 30-day return policy..." â”‚
â”‚  "[Source: FAQ.pdf]"                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Key Files and What They Do

| File | Uses | Purpose |
|------|------|---------|
| `chunking_service.py` | **LangChain** | Splits documents into chunks |
| `vector_store.py` | **Pinecone** | Stores and searches embeddings |
| `embedding_service.py` | OpenAI | Creates meaning fingerprints |
| `document_store_service.py` | LangChain + Pinecone | Manages documents |
| `rag_retriever_service.py` | All of above | Orchestrates retrieval |

## Performance Numbers

```
Document Upload (one-time):
â”œâ”€ Extract text: ~1 second
â”œâ”€ Chunk with LangChain: ~0.5 seconds
â”œâ”€ Create embeddings: ~2 seconds (OpenAI API)
â””â”€ Store in Pinecone: ~0.5 seconds
   Total: ~4 seconds for 10-page PDF

Query (every time):
â”œâ”€ Create query embedding: ~0.1 seconds
â”œâ”€ Search Pinecone: ~0.05 seconds (super fast!)
â”œâ”€ Search database: ~0.05 seconds
â””â”€ Generate response: ~1-2 seconds
   Total: ~1.2-2.2 seconds
```

## Cost Breakdown

```
Per 1000 queries:
â”œâ”€ Embeddings (OpenAI): $0.02
â”œâ”€ Vector storage (Pinecone): $0.01
â”œâ”€ LLM generation (OpenAI): $0.50
â””â”€ Total: ~$0.53 per 1000 queries
```

## Why This Architecture?

1. **LangChain** = Don't reinvent the wheel
   - Text splitting is hard (sentence boundaries, context)
   - LangChain solved this already
   - Battle-tested by thousands of companies

2. **Pinecone** = Speed and scale
   - Searching millions of vectors in milliseconds
   - Handles tenant isolation with namespaces
   - Managed service (no infrastructure headaches)

3. **Together** = Best of both worlds
   - LangChain prepares data perfectly
   - Pinecone stores and retrieves lightning-fast
   - We focus on business logic, not infrastructure

## Summary (TL;DR)

**LangChain:**
- ğŸ”§ Tool for splitting text smartly
- ğŸ“ Used in: `chunking_service.py`
- ğŸ¯ Purpose: Break documents into AI-friendly chunks

**Pinecone:**
- ğŸ“š Smart library for storing meaning fingerprints
- ğŸ“ Used in: `vector_store.py`, `document_store_service.py`
- ğŸ¯ Purpose: Find relevant information super fast

**Together:**
1. LangChain chunks the document
2. OpenAI creates embeddings (meaning fingerprints)
3. Pinecone stores embeddings
4. Customer asks question
5. Pinecone finds relevant chunks
6. AI generates answer using chunks
7. Customer gets accurate response!

**The Magic:** Instead of AI guessing, it looks up the answer in your documents! ğŸ©âœ¨
